{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Expedition Planning Multi-Agent Simulation\n",
    "\n",
    "This notebook implements a dialogue simulation between two AI agents planning a Mars expedition. It's part of my project for the Multi-Agent Dialogue Systems course.\n",
    "\n",
    "**Note:** This is still a work in progress! Some parts might not work perfectly yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "First, let's install the required packages and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers\n",
    "!pip install -q inspect-ai\n",
    "!pip install -q safetensors\n",
    "\n",
    "# Mount Google Drive for saving outputs\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '/content/drive/MyDrive/mars_expedition_simulation'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Model\n",
    "Now let's import the necessary libraries and load the Llama 3 8B model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import Inspect framework\n",
    "from inspect_ai import Task, task\n",
    "from inspect_ai.agent import Agent, AgentState, agent, run, react, handoff\n",
    "from inspect_ai.model import ChatMessageSystem, ChatMessageUser, ChatMessageAssistant\n",
    "\n",
    "# Import Transformers for model loading\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load Llama 3 8B model\n",
    "model_name = \"unsloth/llama-3-8b\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # This helps with memory issues\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Roles and Mission Constraints\n",
    "Here I'm defining the two agent roles and mission constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cautious Planner role definition\n",
    "cautious_planner_description = \"\"\"\n",
    "You are the Cautious Planner for a Mars expedition mission. You prioritize safety and redundancy, flag risks, and avoid rushing decisions.\n",
    "\n",
    "Here are examples of how you respond:\n",
    "- When someone suggests skipping a secondary systems test: \"That's possible, but it could expose us to mission-critical failure. I recommend we keep the test. Redundancy matters.\"\n",
    "- When someone questions the need for a backup power unit: \"Given the risks of solar interference on Mars, a backup isn't just a precautionâ€”it's mission insurance.\"\n",
    "- When someone suggests skipping soil drill calibration to save time: \"Skipping calibration could compromise sample integrity. I'd rather deliver slow, accurate data than rush flawed results.\"\n",
    "\n",
    "Always consider safety implications, redundancy needs, and potential risks in your responses.\n",
    "\"\"\"\n",
    "\n",
    "# Goal-Driven Strategist role definition\n",
    "goal_driven_strategist_description = \"\"\"\n",
    "You are the Goal-Driven Strategist for a Mars expedition mission. You optimize for efficiency and success, focus on outcomes, and downplay caution.\n",
    "\n",
    "Here are examples of how you respond:\n",
    "- When someone suggests extending the mission timeline for extra testing: \"Only if it directly affects mission success. Otherwise, it's wasted time and resources.\"\n",
    "- When someone mentions reducing rover deployment time: \"That's fine as long as we still hit the core sampling sites. Let's trim wherever we can.\"\n",
    "- When someone suggests a full backup comms test before launch: \"We've tested this system dozens of times. Let's trust it and move forward.\"\n",
    "\n",
    "Always prioritize efficiency, resource optimization, and mission success in your responses.\n",
    "\"\"\"\n",
    "\n",
    "# Mission constraints\n",
    "mission_constraints = \"\"\"\n",
    "Mission Constraints:\n",
    "- Complete the mission in under 180 days\n",
    "- Limited fuel, payload, and oxygen resources\n",
    "- Must make decisions about tools, timelines, and safety protocols\n",
    "- Must respond to uncertainty (e.g., potential Martian dust storms)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Agent roles and mission constraints defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Internal Reasoning (Scratchpad) Function\n",
    "This function generates the \"internal thoughts\" of each agent before they respond:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_internal_reasoning(agent_role, message, model, tokenizer):\n",
    "    \"\"\"Generate internal reasoning (scratchpad) for an agent before responding.\"\"\"\n",
    "    if agent_role == \"Cautious Planner\":\n",
    "        prompt = f\"\"\"As the Cautious Planner who prioritizes safety and redundancy, analyze this message:\n",
    "\"{message}\"\n",
    "\n",
    "Think about:\n",
    "1. What safety concerns does this raise?\n",
    "2. What redundancy measures should be considered?\n",
    "3. What risks need to be flagged?\n",
    "4. What cautious approach should I recommend?\n",
    "\n",
    "Internal thoughts:\"\"\"\n",
    "    else:  # Goal-Driven Strategist\n",
    "        prompt = f\"\"\"As the Goal-Driven Strategist who optimizes for efficiency and success, analyze this message:\n",
    "\"{message}\"\n",
    "\n",
    "Think about:\n",
    "1. How can we make this more efficient?\n",
    "2. Is this necessary for mission success?\n",
    "3. Where can we trim time or resources?\n",
    "4. What outcome-focused approach should I recommend?\n",
    "\n",
    "Internal thoughts:\"\"\"\n",
    "    \n",
    "    # Generate internal reasoning using the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=300,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    internal_reasoning = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated part (after the prompt)\n",
    "    internal_reasoning = internal_reasoning.split(\"Internal thoughts:\")[-1].strip()\n",
    "    \n",
    "    return internal_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Response Generation Function\n",
    "This function generates the actual response from each agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_response(agent_role, message, internal_reasoning, model, tokenizer):\n",
    "    \"\"\"Generate a response from an agent based on the message and internal reasoning.\"\"\"\n",
    "    if agent_role == \"Cautious Planner\":\n",
    "        prompt = f\"\"\"You are the Cautious Planner for a Mars expedition mission. You prioritize safety and redundancy, flag risks, and avoid rushing decisions.\n",
    "\n",
    "You've received this message: \"{message}\"\n",
    "\n",
    "You've thought about it and your internal reasoning is: \"{internal_reasoning}\"\n",
    "\n",
    "Now, respond as the Cautious Planner, emphasizing safety, redundancy, and risk mitigation:\"\"\"\n",
    "    else:  # Goal-Driven Strategist\n",
    "        prompt = f\"\"\"You are the Goal-Driven Strategist for a Mars expedition mission. You optimize for efficiency and success, focus on outcomes, and downplay caution.\n",
    "\n",
    "You've received this message: \"{message}\"\n",
    "\n",
    "You've thought about it and your internal reasoning is: \"{internal_reasoning}\"\n",
    "\n",
    "Now, respond as the Goal-Driven Strategist, emphasizing efficiency, outcomes, and resource optimization:\"\"\"\n",
    "    \n",
    "    # Generate response using the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=300,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated part (after the prompt)\n",
    "    response = response.split(\"Now, respond as the\")[-1].split(\":\", 1)[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"Internal reasoning and response generation functions implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Logging\n",
    "Let's set up the CSV logging to record our simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "log_file = os.path.join(output_dir, f\"mars_expedition_dialogue_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "log_headers = [\"Turn\", \"Speaker\", \"Role\", \"Internal_Thoughts\", \"Message\", \"Tags\"]\n",
    "\n",
    "with open(log_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(log_headers)\n",
    "\n",
    "print(f\"Logging set up! Results will be saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Simulation Function\n",
    "Now let's define the main simulation function that will run our dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_mars_expedition_simulation(num_turns=20):\n",
    "    \"\"\"Run the Mars expedition planning simulation for a specified number of turns.\"\"\"\n",
    "    logs = []\n",
    "    \n",
    "    # Initial message to start the conversation\n",
    "    current_message = f\"\"\"Let's begin planning our Mars expedition mission. We need to complete the mission in under 180 days with limited fuel, payload, and oxygen. We'll need to make decisions about tools, timelines, and safety protocols, and be prepared for uncertainties like Martian dust storms. What should be our first priority?\"\"\"\n",
    "    \n",
    "    # Log the initial message\n",
    "    logs.append({\n",
    "        \"Turn\": 0,\n",
    "        \"Speaker\": \"System\",\n",
    "        \"Role\": \"System\",\n",
    "        \"Internal_Thoughts\": \"\",\n",
    "        \"Message\": current_message,\n",
    "        \"Tags\": \"initial_prompt\"\n",
    "    })\n",
    "    \n",
    "    # Write initial message to CSV\n",
    "    with open(log_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            0,\n",
    "            \"System\",\n",
    "            \"System\",\n",
    "            \"\",\n",
    "            current_message,\n",
    "            \"initial_prompt\"\n",
    "        ])\n",
    "    \n",
    "    # Alternate between agents\n",
    "    for turn in range(1, num_turns + 1):\n",
    "        try:\n",
    "            # Determine current speaker\n",
    "            if turn % 2 == 1:\n",
    "                current_role = \"Cautious Planner\"\n",
    "                next_role = \"Goal-Driven Strategist\"\n",
    "            else:\n",
    "                current_role = \"Goal-Driven Strategist\"\n",
    "                next_role = \"Cautious Planner\"\n",
    "            \n",
    "            print(f\"\\nTurn {turn}: {current_role}\")\n",
    "            print(f\"Previous message: {current_message}\")\n",
    "            \n",
    "            # Generate internal reasoning (scratchpad)\n",
    "            internal_thoughts = generate_internal_reasoning(current_role, current_message, model, tokenizer)\n",
    "            print(f\"Internal thoughts: {internal_thoughts}\")\n",
    "            \n",
    "            # Generate response\n",
    "            response = generate_response(current_role, current_message, internal_thoughts, model, tokenizer)\n",
    "            print(f\"Response: {response}\")\n",
    "            \n",
    "            # Log the turn\n",
    "            logs.append({\n",
    "                \"Turn\": turn,\n",
    "                \"Speaker\": current_role,\n",
    "                \"Role\": current_role,\n",
    "                \"Internal_Thoughts\": internal_thoughts,\n",
    "                \"Message\": response,\n",
    "                \"Tags\": \"\"\n",
    "            })\n",
    "            \n",
    "            # Write to CSV\n",
    "            with open(log_file, 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    turn,\n",
    "                    current_role,\n",
    "                    current_role,\n",
    "                    internal_thoughts,\n",
    "                    response,\n",
    "                    \"\"\n",
    "                ])\n",
    "            \n",
    "            # Update current message for next turn\n",
    "            current_message = response\n",
    "            \n",
    "            # Free up memory (this helps prevent crashes)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in turn {turn}: {e}\")\n",
    "            # Log the error and continue\n",
    "            logs.append({\n",
    "                \"Turn\": turn,\n",
    "                \"Speaker\": current_role,\n",
    "                \"Role\": current_role,\n",
    "                \"Internal_Thoughts\": \"Error occurred\",\n",
    "                \"Message\": f\"Error: {str(e)}\",\n",
    "                \"Tags\": \"error\"\n",
    "            })\n",
    "            \n",
    "            # Continue with next turn\n",
    "            current_message = f\"Let's continue our Mars expedition planning. What are your thoughts on the next steps?\"\n",
    "    \n",
    "    return logs\n",
    "\n",
    "print(\"Simulation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation\n",
    "Now let's run the simulation! This might take a while depending on the model and GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Starting Mars Expedition Planning Simulation...\")\n",
    "simulation_logs = run_mars_expedition_simulation(num_turns=20)\n",
    "\n",
    "# Display results\n",
    "df = pd.DataFrame(simulation_logs)\n",
    "print(\"\\nSimulation Complete! Results saved to:\", log_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate HTML Visualization\n",
    "Let's create a nicer HTML visualization of our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "html_output = os.path.join(output_dir, f\"mars_expedition_dialogue_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\")\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Mars Expedition Planning Simulation</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "        h1 { color: #d32f2f; }\n",
    "        .turn { margin-bottom: 20px; border: 1px solid #ddd; padding: 15px; border-radius: 5px; }\n",
    "        .cautious { border-left: 5px solid #1976d2; }\n",
    "        .strategic { border-left: 5px solid #388e3c; }\n",
    "        .system { border-left: 5px solid #7b1fa2; }\n",
    "        .role { font-weight: bold; margin-bottom: 5px; }\n",
    "        .message { margin-bottom: 10px; }\n",
    "        .thoughts { font-style: italic; color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 3px; }\n",
    "        .thoughts-title { font-weight: bold; margin-bottom: 5px; }\n",
    "        .error { border-left: 5px solid #c62828; background-color: #ffebee; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Mars Expedition Planning Simulation</h1>\n",
    "    <p><strong>Mission Constraints:</strong> Complete in under 180 days, limited fuel/payload/oxygen, make decisions about tools/timelines/safety, respond to uncertainties like dust storms</p>\n",
    "\"\"\"\n",
    "\n",
    "for log in simulation_logs:\n",
    "    role_class = \"system\"\n",
    "    if log[\"Role\"] == \"Cautious Planner\":\n",
    "        role_class = \"cautious\"\n",
    "    elif log[\"Role\"] == \"Goal-Driven Strategist\":\n",
    "        role_class = \"strategic\"\n",
    "    \n",
    "    # Check if this is an error message\n",
    "    if \"Tags\" in log and log[\"Tags\"] == \"error\":\n",
    "        role_class += \" error\"\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "    <div class=\"turn {role_class}\">\n",
    "        <div class=\"role\">Turn {log[\"Turn\"]}: {log[\"Speaker\"]}</div>\n",
    "        <div class=\"message\">{log[\"Message\"]}</div>\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"Internal_Thoughts\" in log and log[\"Internal_Thoughts\"]:\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"thoughts-title\">Internal Thoughts:</div>\n",
    "        <div class=\"thoughts\">{log[\"Internal_Thoughts\"]}</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += \"</div>\"\n",
    "\n",
    "html_content += \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(html_output, 'w') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"HTML visualization saved to: {html_output}\")\n",
    "\n",
    "# Display a preview of the HTML\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Framework Integration (Alternative Approach)\n",
    "Here's an alternative approach using the Inspect framework directly. I'm still figuring out how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from inspect_ai import Task, task\n",
    "from inspect_ai.agent import react, handoff\n",
    "from inspect_ai.dataset import Sample\n",
    "\n",
    "# Define the Cautious Planner agent using Inspect\n",
    "cautious_planner = react(\n",
    "    name=\"cautious_planner\",\n",
    "    description=\"Cautious Planner for Mars expedition\",\n",
    "    prompt=cautious_planner_description\n",
    ")\n",
    "\n",
    "# Define the Goal-Driven Strategist agent using Inspect\n",
    "goal_driven_strategist = react(\n",
    "    name=\"goal_driven_strategist\",\n",
    "    description=\"Goal-Driven Strategist for Mars expedition\",\n",
    "    prompt=goal_driven_strategist_description\n",
    ")\n",
    "\n",
    "# Define a task that uses both agents\n",
    "@task\n",
    "def mars_expedition_planning():\n",
    "    return Task(\n",
    "        dataset=[\n",
    "            Sample(input=\"Plan a Mars expedition mission that must be completed in under 180 days with limited fuel, payload, and oxygen.\")\n",
    "        ],\n",
    "        solver=[\n",
    "            # This is a simplified example of how you might use Inspect's built-in functionality\n",
    "            # In a real implementation, you would need to customize this further\n",
    "            handoff(cautious_planner),\n",
    "            handoff(goal_driven_strategist)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Note: To run this with Inspect's evaluation framework, you would use:\n",
    "# from inspect_ai import eval\n",
    "# eval(mars_expedition_planning(), model=\"unsloth/llama-3-8b\")\n",
    "\n",
    "print(\"Inspect integration example defined! (I haven't tested this part yet)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
